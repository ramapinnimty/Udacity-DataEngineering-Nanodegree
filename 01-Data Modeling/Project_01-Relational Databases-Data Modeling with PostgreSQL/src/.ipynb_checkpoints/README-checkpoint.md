# Sparkify
A startup called `Sparkify` wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of *`JSON` logs* on user activity on the app, as well as a directory with *`JSON` metadata* on the songs in their app.

## Project Description
I assumed the role of a Data Engineer to create a Postgres database with tables designed to optimize queries on song play analysis. Particularly, my task was to create a database using Star schema and build a ETL pipeline using my knowledge on Python and SQL to transfer data from the `JSON` files into tables in Postgres.

## Dataset
### Song dataset
This dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in `JSON` format and **contains metadata** about a song and the artist of that song. The files are partitioned by the *first three letters of each song's track ID*.

### Log dataset
This dataset consists of log files in `JSON` format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These **simulate activity logs** from a music streaming app based on specified configurations. The files are partitioned by *year* and *month*.

## Database Schema for Song Play Analysis
Using the `song_data` and `log_data` datasets, I created a **star schema** optimized for queries on song play analysis. This includes the following tables: -

### Fact Table

**songplays** - records in `log_data` associated with song plays i.e. records with the page `NextSong`.
- *songplay_id (SERIAL and hence AUTO_INCREMENT), start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

### Dimension Tables

**users** - users in the app.
- *user_id, first_name, last_name, gender, level*

**songs** - songs in the music database.
- *song_id, title, artist_id, year, duration*

**artists** - artists in the music database.
- *artist_id, name, location, latitude, longitude*

**time** - timestamps of records in `songplays` broken down into specific units.
- *start_time, hour, day, week, month, year, weekday*

## How to run the scripts
```
# Step-1: Create tables
    - `python create_tables.py`
    - [Optional] Run `test.ipynb` to confirm the creation of the tables with the correct columns.

# Step-2: Build ETL pipeline
    - `python etl.py`
    - [Optional] Run `test.ipynb` to confirm the records were successfully inserted into each table.
```

## File structure and description

| Path | Description
| :--- | :----------
| / | Main folder.
| &boxv;&nbsp; &boxvr;&nbsp; [data](#) | Folder containing raw data for the project.
| &boxv;&nbsp; &ensp;&ensp; &boxvr;&nbsp; [song_data](#) | A subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
| &boxv;&nbsp; &ensp;&ensp; &boxvr;&nbsp; [log_data](#) | Log files generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the `song_data` dataset.
&boxvr;&nbsp; [create_tables.py](#) | Drops (if already exists) and then creates our tables. We also need to run this file to reset the tables before each time we run the ETL scripts.
&boxvr;&nbsp; [etl.ipynb](#) | Reads and processes *a single file* from `song_data` and `log_data` and loads the data into tables. It also contains detailed instructions on the ETL process for each of the tables.
&boxvr;&nbsp; [etl.py](#) | Reads and processes *all the files* from `song_data` and `log_data` and loads them into tables.
&boxvr;&nbsp; [README.md](#) | Provides discussion on the project.
&boxvr;&nbsp; [sql_queries.py](#) | Contains all SQL queries, and is imported into the files `create_tables.py`, `etl.ipynb` and `etl.py` above.
&boxvr;&nbsp; [test.ipynb](#) | Displays the first few rows of each table to perform sanity checks on the database.