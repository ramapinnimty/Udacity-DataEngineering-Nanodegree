# Sparkify
A music streaming startup, `Sparkify`, has grown their user base and song database and wants to move their processes and data onto the cloud. Currently their data resides in `S3`, in a directory of *`JSON logs*` on user activity as well as a directory with *`JSON metadata*` on the songs in their app.

## Project Description
I assumed the role of a Data Engineer to apply what I've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. Specifically, my task was to load data from `S3` to staging tables on `Redshift` and execute `SQL` statements to create a set of dimensional tables from the staging tables. By doing this, the analytics team can continue finding insights into what songs their users are listening to.

## Dataset

I'll be working with two datasets that reside in S3 accessible at the links below: -

- Song data: `s3://udacity-dend/song_data`
- Log data: `s3://udacity-dend/log_data`
- Log data json path: `s3://udacity-dend/log_json_path.json`

### Song dataset
This dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in `JSON` format and **contains metadata about a song and the artist of that song**. The files are *partitioned by the first three letters of each song's track ID*.

And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like: 

```
{
    "num_songs": 1, 
    "artist_id": "ARJIE2Y1187B994AB7", 
    "artist_latitude": null, 
    "artist_longitude": null, 
    "artist_location": "", 
    "artist_name": "Line Renaud", 
    "song_id": "SOUPIRU12A6D4FA1E1", 
    "title": "Der Kleine Dompfaff", 
    "duration": 152.92036, 
    "year": 0
}
```

### Log dataset
This dataset consists of log files in `JSON` format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These **simulate activity logs** from a music streaming app based on specified configurations. The files are *partitioned by year and month*.

<div align="center"><img src="log_data_sample.png"/></div>
<br>

## Database Schema for Song Play Analysis
Using the `song_data` and `log_data` datasets, I created a **star schema** optimized for queries on song play analysis which includes the following tables: -

### Fact Table

**fact_songplay** - records in `log_data` associated with song plays i.e. records with the page `NextSong`.
- *songplay_id (SERIAL and hence AUTO_INCREMENT), start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

### Dimension Tables

**dim_user** - users in the app.
- *user_id, first_name, last_name, gender, level*

**dim_song** - songs in the music database.
- *song_id, title, artist_id, year, duration*

**dim_artist** - artists in the music database.
- *artist_id, name, location, latitude, longitude*

**dim_time** - timestamps of records in `fact_songplay` broken down into specific units.
- *start_time, hour, day, week, month, year, weekday*

## How to run the scripts
```
# Step-0: AWS Setup
    - Create an `IAM role` to attach to the Redshift cluster to load data from Amazon S3 buckets.
    - Create a `Subnet group` by navigating through the side menu in Redshift.
    - Create a `Security group` (that controls inbound and outbound traffic) by navigating to the EC2 service to authorize access to the Redshift cluster.
    - Launch a `Redshift cluster` (make sure to enable the `public access`).
    - [LATER] Delete the Redshift cluster when finished to avoid unnecessary costs.
    NOTE: - Please make sure to create the cluster in 'us-west-2' region since the data on S3 is located in that region.

# Step-1: Create tables
    - `python create_tables.py`

# Step-2: Build ETL pipeline
    - `python etl.py`
```

## File structure and description

| Path | Description
| :--- | :----------
| / | Main folder.
&boxvr;&nbsp; [create_tables.py](#) | We'll use this to create the fact and dimension tables for the star schema in Redshift.
&boxvr;&nbsp; [dwh.cfg](#) | Configuration file for the project which is loaded into `create_tables.py`, `sql_queries.py` and `etl.py` files.
&boxvr;&nbsp; [etl.py](#) | Reads and processes *all the files* from `song_data` and `log_data` and loads them into tables.
&boxvr;&nbsp; [README.md](#) | Provides discussion on the project.
&boxvr;&nbsp; [sql_queries.py](#) | Contains all SQL queries, and is imported into the files `create_tables.py` and `etl.py` above.